{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Yf06qHfHROg_"
      },
      "outputs": [],
      "source": [
        "# üì¶ Install dependencies\n",
        "!pip install transformers torch datasets accelerate seqeval rouge-score nltk --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(\"Using device:\", \"CUDA\" if device == 0 else \"CPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bG5cpIGzRSxh",
        "outputId": "9cb27bb7-0770-4de9-9786-98d9af74d050"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment Analysis\n",
        "sentiment_pipe = pipeline(\"sentiment-analysis\",\n",
        "                          model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "                          device=device)\n",
        "\n",
        "# Named Entity Recognition (NER)\n",
        "ner_pipe = pipeline(\"ner\",\n",
        "                    model=\"dslim/bert-base-NER\",\n",
        "                    aggregation_strategy=\"simple\",\n",
        "                    device=device)\n",
        "\n",
        "# Question Answering (QA)\n",
        "qa_pipe = pipeline(\"question-answering\",\n",
        "                   model=\"distilbert-base-cased-distilled-squad\",\n",
        "                   device=device)\n",
        "\n",
        "# Summarization\n",
        "summ_pipe = pipeline(\"summarization\",\n",
        "                     model=\"facebook/bart-large-cnn\",\n",
        "                     device=device)\n",
        "\n",
        "print(\"‚úÖ All pipelines loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofH6kchJRcWT",
        "outputId": "dba45071-2e63-4abf-95c0-1458e62f6cba"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All pipelines loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def nlp_task_selector(task, text, question=None):\n",
        "    if not text.strip():\n",
        "        return \"‚ö†Ô∏è Please enter some text.\"\n",
        "\n",
        "    # SENTIMENT\n",
        "    if task == \"Sentiment Classification\":\n",
        "        result = sentiment_pipe(text)[0]\n",
        "        return f\"üß© Sentiment: {result['label']} ({result['score']:.2f})\"\n",
        "\n",
        "    # NER\n",
        "    elif task == \"Named Entity Recognition\":\n",
        "        entities = ner_pipe(text)\n",
        "        if not entities:\n",
        "            return \"No named entities found.\"\n",
        "        return \"\\n\".join([f\"{e['word']} ‚Üí {e['entity_group']} ({e['score']:.2f})\" for e in entities])\n",
        "\n",
        "    # QUESTION ANSWERING\n",
        "    elif task == \"Question Answering\":\n",
        "        if not question.strip():\n",
        "            return \"‚ö†Ô∏è Please enter a question.\"\n",
        "        result = qa_pipe(question=question, context=text)\n",
        "        return f\"üí¨ Answer: {result['answer']} ({result['score']:.2f})\"\n",
        "\n",
        "    # SUMMARIZATION\n",
        "    elif task == \"Summarization\":\n",
        "        summary = summ_pipe(text, max_length=80, min_length=30, do_sample=False)[0]['summary_text']\n",
        "        return f\"üì∞ Summary:\\n{summary}\"\n",
        "\n",
        "    else:\n",
        "        return \"Invalid task selected.\""
      ],
      "metadata": {
        "id": "QYgsQPnbRuKn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_options = [\"Sentiment Classification\", \"Named Entity Recognition\", \"Question Answering\", \"Summarization\"]\n",
        "\n",
        "with gr.Blocks(theme=\"soft\") as demo:\n",
        "    gr.Markdown(\"## üß† BERT for NLP Tasks (Interactive Demo)\")\n",
        "    gr.Markdown(\"Select a task below and input your text. Uses pre-trained BERT-based models from Hugging Face.\")\n",
        "\n",
        "    task = gr.Radio(task_options, label=\"Choose NLP Task\")\n",
        "    text = gr.Textbox(label=\"Enter Text / Context\", lines=5, placeholder=\"Type or paste your text here...\")\n",
        "    question = gr.Textbox(label=\"Enter Question (only for QA task)\", placeholder=\"e.g. Who invented transformers?\")\n",
        "    output = gr.Textbox(label=\"Result\", lines=5)\n",
        "\n",
        "    btn = gr.Button(\"Run Task üöÄ\")\n",
        "    btn.click(nlp_task_selector, inputs=[task, text, question], outputs=output)\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "6-WdqyBGR2iD",
        "outputId": "d0577d60-0160-4b42-867c-4c16e60079ab"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c7164602208b3ead5d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c7164602208b3ead5d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "This notebook demonstrates how BERT-based Transformer models can perform key NLP tasks such as Sentiment Classification, Named Entity Recognition (NER), Question Answering (QA), and Text Summarization using PyTorch and Hugging Face Transformers.\n",
        "\n",
        "It also uses Gradio to build an interactive interface, allowing users to enter text, select a task, and instantly see model predictions ‚Äî all without writing additional code.\n",
        "\n",
        "Overall, this lab shows how pre-trained Transformer models can be easily applied for real-world language understanding and generation tasks."
      ],
      "metadata": {
        "id": "IhWUOEw-qLk7"
      }
    }
  ]
}